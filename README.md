# Deep Learning Framework
Implementation of commonly used layers in Deep Learning in Python 3.7.


Folder -> Layers:

1) BatchNormalization.py: Contains the implementation of the Batch normalization layer.

2) Conv.py: Contains the implementation of the Convolutional layer which is the main building block of a Convolutional Neural Network (CNN).

3) Dropout.py: Contains the implementation of the commonly used regularization technique called Dropout.

4) Flatten.py: Contains the implementation of the layer which flattens the tensor.

5) FullyConnected.py: Contains the implementation of the Linear (inner product) layer.

6) LSTM.py: Contains the implementation of the Long short term memory (LSTM) layer.

7) Pooling.py: Contains the implementation of the Max pooling layer.

8) RNN.py: Contains the implementation of the standard Recurrent Neural Network (RNN).

9) ReLU.py: Contains the implementation of the famously used non-linearity called ReLU.

10) Sigmoid.py: Contains the implementation of the Sigmoid non-linearity.

11) TanH.py: Contains the implementation of the TanH non-linearity.

12) SoftMax.py: Contains the implementation of the softmax activation function.

13) Base.py: Used a base class for the regularization layers (Batch normalization and Dropout) for setting their states in the training and evaluation phase.




Folder -> optimizers:

1) Constraints.py: Contains the implementation of the L1 and L2 weight decay.

2) Optimizers.py: Contains the implementation of the commonly used optimizers in Deep learning (SGD, SGD with momentum and Adam).




Folder -> initializers:

1) Initializers.py: Contains the implementation of the famous weight initialization schemes used in Deep learning.

